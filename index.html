<html contenteditable=""><script src="moz-extension://a65e5e43-7b2f-402e-961b-b8791a61673c/inpage.js" id="argent-x-extension" data-extension-id="{51e0c76c-7dbc-41ba-a45d-c579be84301b}"></script><head>
<title>
tinychain: A simple and powerful blockchain framework
</title>
<style>
  body {
    font-family:'Lucida Console', monospace
  }
</style>
</head><body>
<h1>welcome to tinychain<br></h1><div>Tinychain is an ultra-lightweight blockchain core.</div><div><br></div><div>In under 1000 LOC:</div><div>- cryptography<br></div><div>- transactions</div><div>- consensus - Nakamoto POW, Tendermint PoS</div><div>- VM's - Lisp, Brainfuck, EVM</div><div>- state machine</div><div>- gas markets<br></div><div>- protocol, RPC, and P2P networking</div><div><br></div><h1></h1><hr>
<h2>why tinychain?</h2><div>Because we just need to **build**</div><div><br></div><div>We were sick and tired of trying to hack on new blockchain designs and finding it takes over 5hrs of guides just to get one of these things working.</div><div><br></div><div>It should not be this hard.<br></div><div><br></div><div>Tinychain is like an F1, OP Stack is like a kayak.</div><div><br></div><div>Seriously, it's 2023. We should not be publishing frameworks that mention the correct packages you have to install. We have Docker.</div><div><br></div><div>Building a new blockchain should be as easy as Next.js. Just install, run one command, and it's running.</div><div><br></div><div>That's what the tinychain is about. Let the devs do what they do best - building cool stuff.<br></div><div><br></div><hr>

<h2>the first blockchain - brainnet.<br></h2>

If you come work here, this is a list to get started. Assumes some familiarity with the <a href="https://github.com/geohot/tinygrad">tinygrad</a> codebase.

<h3>Getting AMD on MLPerf</h3>
There's a <a href="https://github.com/users/geohot/projects/2">GitHub project</a> for this. Can you make the models train?<br>
$500 per model to get tinygrad to train from scratch to the required error within 12 hours.<br>
Learn more in the #mlperf-bounties channel on our <a href="https://discord.gg/ZjZadyC7PK">Discord</a><br>

<h3>Fastest Stable Diffusion and LLaMA on M1</h3>
For LLaMA: Write the specializedjit and support int8.<br>
For Stable Diffusion: Clean up to use BS=2, write winograd conv and kernel search. Target is 0.324 s/it on M1 Max.<br>

<h3>Qualcomm DSP Support</h3>
Switch over comma's DSP based driver monitoring model to tinygrad.<br>
A new architecture to port!<br>

<h3>Max out all the GEMMs</h3>
NVIDIA: Support local memory tensors. Some work in <a href="https://github.com/geohot/tinygrad/pull/557">this branch</a>.<br>
Apple M1: Support using Tensor Cores and simdgroup_float8x8. Some work in <a href="https://github.com/geohot/tinygrad/pull/728">this branch</a>.<br>
AMD: Write assembly backend for the linearizer<br>

<h3>Multi GPU training</h3>
Trees, rings, and collnets oh my! AMD gets us P2P for free.

<h3>Kernel Search</h3>
There's a ton of ways to generate equivalent kernels, enumerate them and search them.<br>
Maybe with ML based search running in tinygrad. Self improvement?

<h3>Kernel Combining (lazy.py)</h3>
ResNets don't fuse as nicely as they could because which reduce do you fuse the elementwise with?<br>
BatchNorm at training and GroupNorm always is 4 kernels. Merge them!
<br>

<hr>

<h2>The tinybox</h2>
<li>738 FP16 TFLOPS</li>
<li>144 GB GPU RAM</li>
<li>5.76 TB/s RAM bandwidth</li>
<li>30 GB/s model load bandwidth (big llama loads in around 4 seconds)</li>
<li>AMD EPYC CPU</li>
<li>2x 1600W (two 120V outlets, can power limit for less)</li>
<li>Runs 65B FP16 LLaMA out of the box (using tinygrad, subject to software development risks)</li>
<li><b>$15,000</b></li>
<br>
<a href="https://buy.stripe.com/5kAaGL6lk9uX9nW144">Preorder a tinybox today! (just $100)</a><br><br>
(specs subject to change. all preorders fully refundable until your tinybox ships. price doesn't include shipping. estimated timeline 2-6 months)
<br>
<hr>

<h2>FAQ:</h2>

<em>Is tinygrad used anywhere?</em><br><br>
tinygrad is used in <a href="https://github.com/commaai/openpilot">openpilot</a> to run the driving model on the Snapdragon 845 GPU. It replaces <a href="https://developer.qualcomm.com/sites/default/files/docs/snpe/overview.html">SNPE</a>, is faster, supports loading onnx files, supports training, and allows for attention (SNPE only allows fixed weights).
<br><br><br>

<em>Is tinygrad inference only?</em><br><br>
No! It supports full forward and backward passes with autodiff. <a href="https://github.com/geohot/tinygrad/blob/master/tinygrad/mlops.py">This</a> is implemented at a level of abstraction higher than the accelerator specific code, so a tinygrad port gets you this for free.
<br><br><br>

<em>How can I use tinygrad for my next ML project?</em><br><br>
Follow the installation instructions on <a href="https://github.com/geohot/tinygrad">the tinygrad repo</a>. It has a similar API to PyTorch, yet simpler and more refined. Less stable though while tinygrad is in alpha, so be warned, though it's been fairly stable for a while.
<br><br><br>

<em>When will tinygrad leave alpha?</em><br><br>
When we can reproduce a common set of papers on 1 NVIDIA GPU 2x faster than PyTorch. We also want the speed to be good on the M1. ETA, Q2 next year.
<br><br><br>

<em>How is tinygrad faster than PyTorch?</em><br><br>
For most use cases it isn't yet, but it will be. It has three advantages:
<li>It compiles a custom kernel for every operation, allowing extreme shape specialization.</li>
<li>All tensors are lazy, so it can aggressively fuse operations.</li>
<li>The backend is 10x+ simpler, meaning optimizing one kernel makes everything fast.</li>
<br><br>

<em>How can the tiny corp work for me?</em><br><br>

Email me, george@tinygrad.org. We are looking for contracts and sponsorships to improve various aspects of tinygrad.
<br><br><br>

<em>How can I work for the tiny corp?</em><br><br>

See <b>hiring</b> above. Contributions to <a href="https://github.com/geohot/tinygrad">tinygrad</a> on GitHub always welcome, and a good way to get hired.
<br><br><br>

<em>Can I invest in the tiny corp?</em><br><br>

Invest with your PRs.

<br><br><br>

<em>What's the goal of the tiny corp?</em><br><br>

To accelerate. We will commoditize the petaflop and enable AI for everyone.

<div id="sourcegraph-app-background" data-platform="firefox-extension" data-version="23.10.9.2250" style="display: none;"></div></body></html>
